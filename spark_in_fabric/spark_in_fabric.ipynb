{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This notebook contains various goodies on how Apache Spark works and how to use it in Microsoft Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe (with a schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "data2 = [\n",
    "    (\"Sergio\", \"Marquina\", \"Professor\", \"10001\", \"M\", 1000000),\n",
    "    (\"Raquel\", \"Murillo\", \"Lisbon\", \"10002\", \"F\", 75000),\n",
    "    (\"Andres\", \"de Fonollosa\", \"Berlin\", \"10003\", \"M\", 85000),\n",
    "    (\"Agata\", \"Jimenez\", \"Nairobi\", \"10004\", \"F\", 95000),\n",
    "    (\"Anibal\", \"Cortes\", \"Rio\", \"10005\", \"M\", 110000)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"middlename\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    " \n",
    "# creation of the dataframe using the above defined schema\n",
    "df = spark.createDataFrame(data=data2,schema=schema)\n",
    "\n",
    "# printing the schema of the dataframe\n",
    "df.printSchema() \n",
    "\n",
    "# showing the datatypes of the columns\n",
    "df.dtypes\n",
    "\n",
    "# displaying the dataframe (keep in mind there is a limit in number of rows that is displayed)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in files / Writing away to files\n",
    "\n",
    "- When reading files without specifying a schema or without letting spark infer a schema the default type of all your columns will be String\n",
    "- Better to use inferschema = True or specify your own schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the path to our file (fe. Files section of a Fabric lakehouse)\n",
    "csv_path = 'Files/property-sales.csv' \n",
    "\n",
    "# Read a csv file from Files/property-sales.csv\n",
    "df_csv = spark.read.csv(csv_path, header=True) \n",
    "\n",
    "# or if we want spark to do some work for us like telling what data type is in each column and in that way infer the schema\n",
    "df_csv = spark.read.csv(csv_path, header=True, inferSchema=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing dataframes to Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call write.json() method to write the dataframe to a json file\n",
    "# the mode parameter is set to 'overwrite' to overwrite the file if it already exists\n",
    "\n",
    "df_csv.write.json(\"Files/json/property-sales.json\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.json('Files/json/property-sales.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing dataframes to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.write.parquet('Files/parquet/property-sales2.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in multiple parquet files (with metadata)\n",
    "\n",
    "- Spark provides us with all the file metadata in a 'hidden' column that we can add to our dataframe using _metadata. This metadata contains:\n",
    "    - file_modification_time\n",
    "    - row_index\n",
    "    - file_name\n",
    "    - file_size\n",
    "    - file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the parquet files, then add the _metadata column \n",
    "df_all_parquet_plus_metadata = spark.read\\\n",
    "    .parquet('Files/parquet/*.parquet')\\\n",
    "    .select(\"*\", \"_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing to Fabric Lakehouse tables\n",
    "\n",
    "- no spaces or special characters in columnnames! -> use ``` .withColumnRenamed ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing column names to allow write to Lakehouse tables\n",
    "df = df.withColumnRenamed(\"SalePrice ($)\",\"SalePrice_USD\")\\\n",
    "        .withColumnRenamed(\"Address \", \"Address\")\\\n",
    "        .withColumnRenamed(\"City \", \"City\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DF to Table, with different 'modes'\n",
    "\n",
    "- Using ``` .saveAsTable ```, we save the DataFrame as a ```Managed Table``` (Spark terminology) - meaning both the metadata and the data is managed by Spark.\n",
    "- With a managed table, a SQL command such as DROP TABLE table_name deletes both the metadata and the data. \n",
    "- With an unmanaged table, the same command will delete only the metadata, not the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table_name = 'PropertySales'\n",
    "\n",
    "# use saveAsTable to save as a Managed Table\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(delta_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are four different write 'modes' \n",
    "\n",
    "# append the new dataframe to the existing Table\n",
    "df.write.mode(\"append\").format(\"delta\").saveAsTable(delta_table_name)\n",
    "\n",
    "# overwrite existing Table with new DataFrame\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(delta_table_name)\n",
    "\n",
    "# Throw error if data already exists\n",
    "df.write.mode(\"error\").format(\"delta\").saveAsTable(delta_table_name)\n",
    "\n",
    "# Fail silently if data already exists \n",
    "df.write.mode(\"ignore\").format(\"delta\").saveAsTable(delta_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing an unmanaged table\n",
    "\n",
    "- use ``` .save ``` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unmanaged table\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(path=\"Files/delta/unmanaged.delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from table in Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM SparkSeptember.propertysales LIMIT 1000\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important pyspark operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of viewing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()\n",
    "\n",
    "display(df)\n",
    "\n",
    "# show the first 2 rows of the dataframe\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "\n",
    "# showing the datatypes of the columns (in a list of tuples)\n",
    "df.dtypes\n",
    "\n",
    "# show the schema\n",
    "df.schema\n",
    "\n",
    "# this is sometimes useful as we might have to do something like this \n",
    "source_schema = df.schema\n",
    "\n",
    "# this saves us having to explicitly write out our the schema for a new df, if we have one that already exists. \n",
    "new_df_with_existing_schema = spark.read.csv( schema=source_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see that columns we have: \n",
    "df.columns\n",
    "\n",
    "#selecting just a single column \n",
    "df.select('Type').show()\n",
    "\n",
    "#renaming existing columns \n",
    "df = df.withColumnRenamed('Address ', 'Address')\n",
    "df.select('Address').show()\n",
    "\n",
    "# selecting a few columns \n",
    "df.select(['Address','Type']).show()\n",
    "\n",
    "# adding a new column\n",
    "df = df.withColumn('2x_SalePrice', df['SalePrice ($)'] * 2)\n",
    "\n",
    "# renaming multiple columns\n",
    "df_new = df.selectExpr(\"Address as ADD\",\"'SalePrice ($)' as SalesPrice_USD\",\"'City ' as MyCity\")\n",
    "\n",
    "# dropping a column\n",
    "df = df.drop('2x_SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering you dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# simple filter condition (pythonic)\n",
    "df.filter(df['City'] == \"New York\").show()\n",
    "\n",
    "# not equal to\n",
    "df.filter(df['City'] != \"New York\").show()\n",
    "\n",
    "# with cols function\n",
    "from pyspark.sql.functions import col\n",
    "df.filter(col(\"City\") == \"New York\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StartsWith, Endswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#startswith \n",
    "df.filter(df.City.startswith(\"L\")).show()\n",
    "\n",
    "#endswith\n",
    "df.filter(df.City.endswith(\"ta\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multple conditions, with AND\n",
    "# where city is not Atlanta and the SalePrice is greater than 400k \n",
    "df.filter((df.City != 'Atlanta') & (df.SalePrice_USD > 400000) ).show()\n",
    "\n",
    "# Multple conditions, with OR\n",
    "# where city is Atlanta OR the city is Los Angeles \n",
    "df.filter((df.City == 'Atlanta') | (df.City == 'Los Angeles') ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is a member of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter df if df.City is in the list cities_we_care_about\n",
    "cities_we_care_about=[\"Atlanta\",\"Los Angeles\"]\n",
    "df.filter(df.City.isin(cities_we_care_about)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df if the Type contains 'House' \n",
    "df.filter(df.Type.contains('House')).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL LIKE filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filer where 'House' appears somewhere in df.Type\n",
    "df.filter(df.Type.like(\"%House%\")).show()\n",
    "\n",
    "# filter where df.Type starts with House...\n",
    "df.filter(df.Type.like(\"House%\")).show()\n",
    "\n",
    "# filter where df.Address endswith avenue\n",
    "df.filter(df.Address.like(\"%avenue\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ways to use SQL expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering using raw WHERE conditions you would use in SQL\n",
    "df.filter(\"City != 'Los Angeles'\").show()\n",
    "\n",
    "df.filter(\"City <> 'Los Angeles'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using df.where()\n",
    "For any of the above functions, you can also use df.where() instead of df.filter() if you prefer - it gives the same result (when using the Spark SQL API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df.City == 'Los Angeles').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group by and AGG functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple aggregate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of rows in a group\n",
    "df.groupBy(\"City\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming aggregated column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, max\n",
    "\n",
    "# method1 to rename a column: using withColumnRenamed() \n",
    "df.groupBy('Agent')\\\n",
    "  .max('SalePrice_USD')\\\n",
    "  .withColumnRenamed('max(SalePrice_USD)','max_sales_price')\\\n",
    "  .show()\n",
    "\n",
    "# method2 to rename a column: using agg() and then alias() \n",
    "df.groupBy(\"Agent\") \\\n",
    "  .agg(max('SalePrice_USD').alias('max_sales_price'))\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Returning multiple aggregates in same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg,max, round\n",
    "df.groupBy(\"City\").agg(\n",
    "    round(max(\"SalePrice_USD\"),0).alias(\"max_sale_price\"), \n",
    "    round(avg(\"SalePrice_USD\"),0).alias(\"avg_sale_price\")\n",
    "    ).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering on an aggregate (like a HAVING clause in SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg,max, round, col\n",
    "\n",
    "df.groupBy(\"City\").agg(\n",
    "    round(max(\"SalePrice_USD\"),0).alias(\"max_sale_price\"), \n",
    "    round(avg(\"SalePrice_USD\"),0).alias(\"avg_sale_price\")\n",
    ").where(col(\"avg_sale_price\") >= 500000)\\\n",
    ".show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping by multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(['City', 'Agent']).avg('SalePrice_USD').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling missing values in Spark Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### most basic/drastic drop NAs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
