{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This notebook contains various goodies on how Apache Spark works and how to use it in Microsoft Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe (with a schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "data2 = [(\"Olena\",\"\",\"Eldridge\",\"36636\",\"M\",90000),\n",
    "    (\"Matthew\",\"J\", \"Munro\",\"28832\",\"M\",45400),\n",
    "    (\"Joffrey\",\"Oway\", \"Roberts\",\"12114\",\"F\",64000),\n",
    "    (\"Lennert\",\"\", \"Dushane\",\"32192\",\"F\",141000),\n",
    "    (\"Jane\",\"Rebecca\",\"Jones\",\"99482\",\"F\",56000)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \n",
    "    StructField(\"firstname\",StringType(),True), \n",
    "    StructField(\"middlename\",StringType(),True), \n",
    "    StructField(\"lastname\",StringType(),True), \n",
    "    StructField(\"id\", StringType(), True), \n",
    "    StructField(\"gender\", StringType(), True), \n",
    "    StructField(\"salary\", IntegerType(), True) \n",
    "  ])\n",
    " \n",
    "# creation of the dataframe using the above defined schema\n",
    "df = spark.createDataFrame(data=data2,schema=schema)\n",
    "\n",
    "# printing the schema of the dataframe\n",
    "df.printSchema() \n",
    "\n",
    "# showing the datatypes of the columns\n",
    "df.dtypes\n",
    "\n",
    "# displaying the dataframe (keep in mind there is a limit in number of rows that is displayed)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in files / Writing away to files\n",
    "\n",
    "- When reading files without specifying a schema or without letting spark infer a schema the default type of all your columns will be String\n",
    "- Better to use inferschema = True or specify your own schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the path to our file (fe. Files section of a Fabric lakehouse)\n",
    "csv_path = 'Files/property-sales.csv' \n",
    "\n",
    "# Read a csv file from Files/property-sales.csv\n",
    "df_csv = spark.read.csv(csv_path, header=True) \n",
    "\n",
    "# or if we want spark to do some work for us like telling what data type is in each column and in that way infer the schema\n",
    "df_csv = spark.read.csv(csv_path, header=True, inferSchema=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing dataframes to Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call write.json() method to write the dataframe to a json file\n",
    "# the mode parameter is set to 'overwrite' to overwrite the file if it already exists\n",
    "\n",
    "df_csv.write.json(\"Files/json/property-sales.json\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.json('Files/json/property-sales.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing dataframes to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.write.parquet('Files/parquet/property-sales2.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in multiple parquet files (with metadata)\n",
    "\n",
    "- Spark provides us with all the file metadata in a 'hidden' column that we can add to our dataframe using _metadata. This metadata contains:\n",
    "    - file_modification_time\n",
    "    - row_index\n",
    "    - file_name\n",
    "    - file_size\n",
    "    - file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the parquet files, then add the _metadata column \n",
    "df_all_parquet_plus_metadata = spark.read\\\n",
    "    .parquet('Files/parquet/*.parquet')\\\n",
    "    .select(\"*\", \"_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing to Fabric Lakehouse tables\n",
    "\n",
    "- no spaces or special characters in columnnames! -> use ``` .withColumnRenamed ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing column names to allow write to Lakehouse tables\n",
    "df = df.withColumnRenamed(\"SalePrice ($)\",\"SalePrice_USD\")\\\n",
    "        .withColumnRenamed(\"Address \", \"Address\")\\\n",
    "        .withColumnRenamed(\"City \", \"City\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DF to Table, with different 'modes'\n",
    "\n",
    "- Using ``` .saveAsTable ```, we save the DataFrame as a 'Managed Table' (Spark terminology) - meaning both the metadata and the data is managed by Spark.\n",
    "- With a managed table, a SQL command such as DROP TABLE table_name deletes both the metadata and the data. \n",
    "- With an unmanaged table, the same command will delete only the metadata, not the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table_name = 'PropertySales'\n",
    "\n",
    "# use saveAsTable to save as a Managed Table\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(delta_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are four different write 'modes' \n",
    "\n",
    "# append the new dataframe to the existing Table\n",
    "df.write.mode(\"append\").format(\"delta\").saveAsTable(delta_table_name)\n",
    "\n",
    "# overwrite existing Table with new DataFrame\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(delta_table_name)\n",
    "\n",
    "# Throw error if data already exists\n",
    "df.write.mode(\"error\").format(\"delta\").saveAsTable(delta_table_name)\n",
    "\n",
    "# Fail silently if data already exists \n",
    "df.write.mode(\"ignore\").format(\"delta\").saveAsTable(delta_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing an unmanaged table\n",
    "\n",
    "- use ``` .save ``` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unmanaged table\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(path=\"Files/delta/unmanaged.delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from table in Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM SparkSeptember.propertysales LIMIT 1000\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
